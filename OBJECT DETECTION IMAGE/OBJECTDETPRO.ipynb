{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OBJECT DETECTION "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEFINITION:\n",
    "Object detection refers to the capability of computer and software systems to locate objects in an image/scene and identify each object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FOLLOWING PACKAGES NEED TO BE INSTALLED :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow\\\n",
    "Numpy\\\n",
    "Scipy\\\n",
    "OpenCV\\\n",
    "Pillow\\\n",
    "Matplotlib\\\n",
    "H5py\\\n",
    "Keras\\\n",
    "ImageAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll be using a trained RetinaNet computer vision model to perform the detection and recognition tasks.This model is trained to detect and recognize 80 different objects, named below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "person, bicycle, car, motorcycle, airplane,\n",
    "bus, train, truck, boat, traffic light, fire hydrant, stop_sign,\n",
    "parking meter, bench, bird, cat, dog, horse, sheep, cow, elephant, bear, zebra,\n",
    "giraffe, backpack, umbrella, handbag, tie, suitcase, frisbee, skis, snowboard,\n",
    "sports ball, kite, baseball bat, baseball glove, skateboard, surfboard, tennis racket,\n",
    "bottle, wine glass, cup, fork, knife, spoon, bowl, banana, apple, sandwich, orange,\n",
    "broccoli, carrot, hot dog, pizza, donot, cake, chair, couch, potted plant, bed,\n",
    "dining table, toilet, tv, laptop, mouse, remote, keyboard, cell phone, microwave,\n",
    "oven, toaster, sink, refrigerator, book, clock, vase, scissors, teddy bear, hair dryer,\n",
    "toothbrush."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the RetinaNet model file and the image you want to detect to the folder that contains the python file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imageai.Detection import ObjectDetection\n",
    "import os\n",
    "\n",
    "execution_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above 3 lines, we imported the ImageAI object detection class in the first line, imported the python os class in the second line and defined a variable to hold the path to the folder where our python file, RetinaNet model file and images are in the third line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracking <tf.Variable 'Variable_5:0' shape=(9, 4) dtype=float32> anchors\n",
      "tracking <tf.Variable 'Variable_6:0' shape=(9, 4) dtype=float32> anchors\n",
      "tracking <tf.Variable 'Variable_7:0' shape=(9, 4) dtype=float32> anchors\n",
      "tracking <tf.Variable 'Variable_8:0' shape=(9, 4) dtype=float32> anchors\n",
      "tracking <tf.Variable 'Variable_9:0' shape=(9, 4) dtype=float32> anchors\n"
     ]
    }
   ],
   "source": [
    "detector = ObjectDetection()\n",
    "detector.setModelTypeAsRetinaNet()\n",
    "detector.setModelPath( os.path.join(execution_path , \"resnet50_coco_best_v2.0.1.h5\"))\n",
    "detector.loadModel()\n",
    "detections = detector.detectObjectsFromImage(input_image=os.path.join(execution_path , \"image.png\"), output_image_path=os.path.join(execution_path , \"imagenew.jpg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the 5 lines of code above, we defined our object detection class in the first line, set the model type to RetinaNet in the second line, set the model path to the path of our RetinaNet model in the third line, load the model into the object detection class in the fourth line, then we called the detection function and parsed in the input image path and the output image path in the fifth line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wine glass  :  51.779890060424805\n",
      "bowl  :  61.894214153289795\n",
      "wine glass  :  84.47420597076416\n",
      "bowl  :  59.523558616638184\n",
      "wine glass  :  59.40147638320923\n",
      "wine glass  :  64.37760591506958\n",
      "bowl  :  60.521143674850464\n",
      "bowl  :  53.86190414428711\n",
      "cup  :  55.068814754486084\n",
      "cup  :  63.58487606048584\n",
      "cup  :  70.49213647842407\n",
      "wine glass  :  83.64768028259277\n",
      "wine glass  :  51.20431184768677\n",
      "wine glass  :  76.5000581741333\n",
      "wine glass  :  69.30323839187622\n",
      "potted plant  :  69.89601850509644\n",
      "potted plant  :  56.77960515022278\n",
      "bottle  :  77.4715781211853\n",
      "wine glass  :  95.35048007965088\n",
      "wine glass  :  90.9719467163086\n",
      "apple  :  58.80863070487976\n",
      "sink  :  58.840274810791016\n"
     ]
    }
   ],
   "source": [
    "for eachObject in detections:\n",
    "    print(eachObject[\"name\"] , \" : \" , eachObject[\"percentage_probability\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above 2 lines of code, we iterate over all the results returned by the detector.detectObjectsFromImage function in the first line, then print out the name and percentage probability of the model on each object detected in the image in the second line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections, extracted_images = detector.detectObjectsFromImage(input_image=os.path.join(execution_path , \"image.png\"), output_image_path=os.path.join(execution_path , \"imagenew.jpg\"), extract_detected_objects=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ImageAI supports many powerful customization of the object detection process. One of it is the ability to extract the image of each object detected in the image. By simply parsing the extra parameter extract_detected_objects=True into the detectObjectsFromImage function as seen below, the object detection class will create a folder for the image objects, extract each image, save each to the new folder created and return an extra array that contains the path to each of the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ImageAI provides many more features useful for customization and production capable deployments for object detection tasks. Some of the features supported are:\n",
    "- Adjusting Minimum Probability: By default, objects detected with a probability percentage of less than 50 will not be shown or reported. You can increase this value for high certainty cases or reduce the value for cases where all possible objects are needed to be detected.\n",
    "- Custom Objects Detection: Using a provided CustomObject class, you can tell the detection class to report detections on one or a few number of unique objects.\n",
    "- Detection Speeds: You can reduce the time it takes to detect an image by setting the speed of detection speed to “fast”, “faster” and “fastest”.\n",
    "- Input Types: You can specify and parse in file path to an image, Numpy array or file stream of an image as the input image.\n",
    "- Output Types: You can specify that the detectObjectsFromImage function should return the image in the form of a file or Numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
